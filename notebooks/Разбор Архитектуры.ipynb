{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9beec46d-f5bd-40c0-8157-a78ca230a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Configure the parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412cb225-f388-4f15-b693-5cb78ffcd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from app.utils import sinusoids\n",
    "from dataclasses import dataclass\n",
    "from app.whisper import (\n",
    "    AudioEncoder, \n",
    "    TextDecoder,\n",
    "    ModelDimensions\n",
    ")\n",
    "\n",
    "from app.utils import (\n",
    "    load_original_whisper_weights,\n",
    "    get_whisper_encoder_keys, \n",
    "    get_whisper_encoder_weigths, \n",
    "    get_whisper_decoder_keys, \n",
    "    get_whisper_decoder_weigths,\n",
    "    compute_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da23c35-743d-4a3c-9315-530f2c7b93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # multi head attention used in the encoder\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        wv = self.qkv_attention(q, k, v)\n",
    "        return self.out(wv)\n",
    "\n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor):\n",
    "        n_batch, n_ctx, n_state = q.size()\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(q.size(0), q.size(1), self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(k.size(0), k.size(1), self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(v.size(0), v.size(1), self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        qk = q @ k\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd5cb38-2a3a-4efe-8e3b-809cce5bda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_mlp, n_state),\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # standard encoder attention block with skip connection\n",
    "        x = x + self.attn(self.attn_ln(x))\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22eee8e3-8806-41ac-b780-f19729e4cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMultiHeadAttentionDecoderSelf(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        kv_cache: Tensor,\n",
    "    ):\n",
    "        # q will always come from the bottom (from previous decoder)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # It is essential to define the batch_indices for the case where the offset is not unique \n",
    "        # batch_indices = torch.arange(x.size(0), device=x.device, dtype=torch.int32)\n",
    "        \n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        # print(f\"key: {key.shape}\")\n",
    "        # print(f\"value: {value.shape}\")\n",
    "        \n",
    "        key_cache = torch.cat([kv_cache[:, self.n_layer, 0, ...], key], dim=1)\n",
    "        value_cache = torch.cat([kv_cache[:, self.n_layer, 1, ...], value], dim=1)\n",
    "\n",
    "        k = key_cache\n",
    "        v = value_cache\n",
    "\n",
    "        wv = self.masked_qkv_attention(q, k, v)\n",
    "        return self.out(wv), key, value\n",
    "\n",
    "    def masked_qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor,\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.size()\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(q.size(0), q.size(1), self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(k.size(0), k.size(1), self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(v.size(0), v.size(1), self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        \n",
    "        qk = q @ k\n",
    "\n",
    "        # Mask padded tokens, they deserve 0 attention score\n",
    "        # padding_mask = (qk == 0)\n",
    "        # qk.masked_fill_(padding_mask, float('-inf')) # -- more advanced, ONNX warnings\n",
    "        \n",
    "        # mask = padding_mask * -65504 # Smallest value for float16\n",
    "        # qk = qk + mask\n",
    "        \n",
    "        # the model expects one token at a time\n",
    "        # if mask is not None:\n",
    "        #     print(\"qk.shape, mask.shape, n_ctx\", qk.shape, mask.shape, n_ctx)\n",
    "        #     qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        \n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9ce901-7f50-440f-92cc-3ef3f2f416e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedMultiHeadAttentionDecoderCross(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        n_layer_cross_k: Tensor,\n",
    "        n_layer_cross_v: Tensor,\n",
    "    ):\n",
    "        # q will always come from the bottom (from previous decoder)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # for corss-attention\n",
    "        k = n_layer_cross_k[self.n_layer, ...]\n",
    "        v = n_layer_cross_v[self.n_layer, ...]\n",
    "        \n",
    "        wv = self.masked_qkv_attention(q, k, v)\n",
    "        return self.out(wv)\n",
    "\n",
    "    def masked_qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor,\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.size()\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(q.size(0), q.size(1), self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(k.size(0), k.size(1), self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(v.size(0), v.size(1), self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        \n",
    "        qk = q @ k\n",
    "        \n",
    "        # the model expects one token at a time\n",
    "        # if mask is not None:\n",
    "        #     print(\"qk.shape, mask.shape, n_ctx\", qk.shape, mask.shape, n_ctx)\n",
    "        #     qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        \n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb513d53-672f-47e6-b4dd-661784b55d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_mlp, n_state),\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # standard encoder attention block with skip connection\n",
    "        x = x + self.attn(self.attn_ln(x))\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939a11f0-d29f-49bf-a868-12fa33e51e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.attn = CachedMultiHeadAttentionDecoderSelf(n_state, n_head, n_layer)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "        self.cross_attn = CachedMultiHeadAttentionDecoderCross(n_state, n_head, n_layer)\n",
    "        self.cross_attn_ln = nn.LayerNorm(n_state)\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_state, n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_mlp, n_state),\n",
    "        )\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        kv_cache: Tensor,\n",
    "        n_layer_cross_k,\n",
    "        n_layer_cross_v\n",
    "    ):\n",
    "        # decoder attn and cross-attn block with skip connection\n",
    "        x1, k, v = self.attn(self.attn_ln(x), kv_cache)\n",
    "        x = x + x1\n",
    "        x = x + self.cross_attn(self.cross_attn_ln(x), n_layer_cross_k, n_layer_cross_v)\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba54563-e2eb-4122-875f-8d0cd0dfcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layers: int, encoder_x: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "        self.encoder_x = encoder_x\n",
    "\n",
    "        # encoder\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_post = nn.LayerNorm(n_state)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                CachedResidualAttentionBlock(n_state, n_head, n_layer)\n",
    "                for n_layer in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n",
    "            the mel spectrogram of the audio\n",
    "        \"\"\"\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # assert x[0].size() == self.positional_embedding.size(), \"incorrect audio shape\"\n",
    "        # x = x + self.positional_embedding\n",
    "        \n",
    "        x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_post(x)\n",
    "        \n",
    "        if self.encoder_x:\n",
    "            return x\n",
    "        \n",
    "        ###   DECODER   ###\n",
    "        n_layer_cross_k_list = []\n",
    "        n_layer_cross_v_list = []\n",
    "        for block in self.decoder:\n",
    "            n_layer_cross_k_list.append(block.cross_attn.key(x))\n",
    "            n_layer_cross_v_list.append(block.cross_attn.value(x))\n",
    "        audio_features = torch.stack(n_layer_cross_k_list), torch.stack(n_layer_cross_v_list)\n",
    "        return (audio_features[0].permute(1, 0, 2, 3), audio_features[1].permute(1, 0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c76e67-337a-40cb-93b1-0450aa4e4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab: int,\n",
    "        n_ctx: int,\n",
    "        n_state: int,\n",
    "        n_head: int,\n",
    "        n_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                CachedResidualAttentionBlock(n_state, n_head, n_layer)\n",
    "                for n_layer in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(n_state)\n",
    "\n",
    "        # mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        # self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, kv_cache: Tensor, n_layer_cross_k: Tensor, n_layer_cross_v: Tensor, offset: Tensor):\n",
    "        \"\"\"\n",
    "        x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n",
    "            the text tokens\n",
    "        \"\"\"\n",
    "\n",
    "        # (b_size, n_layers, audio_lenght, d_model)\n",
    "        n_layer_cross_k = n_layer_cross_k.permute(1, 0, 2, 3)\n",
    "        n_layer_cross_v = n_layer_cross_v.permute(1, 0, 2, 3)\n",
    "        \n",
    "        # offset = kv_cache[0].size(1) if len(kv_cache) > 0 else 0\n",
    "        \n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset] # We always expect a single token at a time in the batch \n",
    "        )\n",
    "\n",
    "        keys = []\n",
    "        values = []\n",
    "        for block in self.blocks:\n",
    "            x, k, v = block(x, kv_cache, n_layer_cross_k, n_layer_cross_v)\n",
    "            keys.append(k)\n",
    "            values.append(v)\n",
    "        \n",
    "        x = self.ln(x)\n",
    "        logits = x @ torch.transpose(self.token_embedding.weight, 0, 1)\n",
    "        keys, values = torch.stack((keys), dim=0), torch.stack((values), dim=0)\n",
    "        return logits, keys.permute(1, 0, 2, 3), values.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303c6d66-52e8-47e9-b5ac-d9960cbf993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_weights = load_original_whisper_weights(file_path='../app/model/medium.pt', device='cpu', )\n",
    "dims = ModelDimensions(**medium_weights['dims'])\n",
    "\n",
    "encoder = AudioEncoder(\n",
    "    n_mels=dims.n_mels,\n",
    "    n_ctx=dims.n_audio_ctx,\n",
    "    n_state=dims.n_audio_state,\n",
    "    n_head=dims.n_audio_head,\n",
    "    n_layers=dims.n_audio_layer,\n",
    ")\n",
    "\n",
    "decoder = TextDecoder(\n",
    "    n_vocab=dims.n_vocab,\n",
    "    n_ctx=dims.n_text_ctx,\n",
    "    n_state=dims.n_text_state,\n",
    "    n_head=dims.n_text_head,\n",
    "    n_layers=dims.n_text_layer \n",
    ")\n",
    "\n",
    "encoder_keys = encoder.state_dict()\n",
    "\n",
    "encoder_needed_keys = get_whisper_encoder_keys(encoder_keys)\n",
    "encoder_weights = get_whisper_encoder_weigths(\n",
    "    encoder_keys,\n",
    "    encoder_needed_keys, \n",
    "    medium_weights\n",
    ")\n",
    "encoder.load_state_dict(encoder_weights)\n",
    "encoder = encoder.to(device).half()\n",
    "encoder = encoder.eval()\n",
    "\n",
    "decoder_keys = decoder.state_dict()\n",
    "\n",
    "decoder_needed_keys = get_whisper_decoder_keys(decoder_keys)\n",
    "decoder_weights = get_whisper_decoder_weigths(\n",
    "    decoder_keys, \n",
    "    decoder_needed_keys, \n",
    "    medium_weights\n",
    ")\n",
    "decoder.load_state_dict(decoder_weights)\n",
    "decoder = decoder.to(device).half()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3a140-6089-469b-8d0d-69c4aeaf9978",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c11f0bf-6bcc-4c47-ad13-bee9e06077dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressed tokens, see SuppressBlank and SuppressTokens class\n",
    "suppress_blanks = [220, 50257]\n",
    "suppress_nonspeech = [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, \n",
    "    93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, \n",
    "    3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, \n",
    "    14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, \n",
    "    32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f59b923a-4621-45f4-bf28-6077f30a8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "from transformers import WhisperTokenizerFast\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f39b0fa-e1df-449d-9001-14882aaf226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./fleurs_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "227586b7-e750-40ff-8ab2-edd167d8c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave, sr = sf.read(df.absolute_path[0])\n",
    "mel = compute_features(wave, sample_rate=sr)\n",
    "mel = mel.unsqueeze(0)\n",
    "b_size = mel.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b394b3f9-91fd-4ac3-93b6-3ad634aaa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_kv_cache(\n",
    "    kv_cache: torch.tensor, \n",
    "    keys: torch.tensor, \n",
    "    values: torch.tensor, \n",
    "    offset: int\n",
    ") -> [torch.tensor, int]:\n",
    "    kv_cache[..., 0, offset, :] = keys.squeeze(2)\n",
    "    kv_cache[..., 1, offset, :] = values.squeeze(2)\n",
    "    offset += 1\n",
    "    return kv_cache, offset\n",
    "\n",
    "\n",
    "def get_token(\n",
    "    logits: torch.tensor\n",
    ") -> torch.tensor:\n",
    "    last = logits[:, -1]\n",
    "    last[:, suppress_nonspeech] = -torch.inf\n",
    "    last = last.argmax(-1, keepdim=True)\n",
    "    return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08034c62-9323-4637-b2c6-928dd20f9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "max_token_sequence = 50\n",
    "tokens = torch.tensor([[50258, 50259, 50359, 50363]] * b_size, dtype=torch.int32).to(mel.device)\n",
    "kv_cache = torch.zeros((b_size, 24, 2, max_token_sequence, 1024), dtype=torch.half, device=mel.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_layer_cross_k, n_layer_cross_v = encoder(mel)\n",
    "    \n",
    "    # Spetial tokens\n",
    "    for token in tokens[0]:\n",
    "        logits, keys, values = decoder(\n",
    "            token.unsqueeze(0).unsqueeze(0),\n",
    "            kv_cache,\n",
    "            n_layer_cross_k,\n",
    "            n_layer_cross_v,\n",
    "            offset\n",
    "        )\n",
    "        kv_cache, offset = update_kv_cache(kv_cache, keys, values, offset)\n",
    "        last = get_token(logits)\n",
    "    tokens = torch.cat([tokens, last], dim=-1)\n",
    "    \n",
    "    \n",
    "    # Start of auto-regressiveness\n",
    "    for i in range(max_token_sequence-len(tokens)):\n",
    "        logits, keys, values = decoder(\n",
    "            last,\n",
    "            kv_cache,\n",
    "            n_layer_cross_k, \n",
    "            n_layer_cross_v, \n",
    "            offset\n",
    "        )\n",
    "        kv_cache, offset = update_kv_cache(kv_cache, keys, values, offset)\n",
    "        last = get_token(logits)\n",
    "        tokens = torch.cat([tokens, last], dim=-1)\n",
    "        \n",
    "        # when to stop\n",
    "        if last.item() == 50257:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35d45706-73c1-4853-97ec-6f06c934f4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|en|><|transcribe|><|notimestamps|> However, due to the slow communication channels styles styles in the West could lag behind by 25 to 30 years.<|endoftext|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc99b6-0a00-4f1d-bf3e-0b5d4123c82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c89672-97a5-49da-9355-8a9162d5c6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0798a7-988a-485c-a068-30b4426ce0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a082957-4884-48e1-b180-4a86c04f101d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7ba3f-9942-4b12-a258-826ec956fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43f582-b63d-4ef4-9572-ad949a2ac2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
